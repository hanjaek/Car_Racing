import os
import gymnasium as gym
import numpy as np
import pygame
from stable_baselines3 import SAC
from stable_baselines3.common.vec_env import DummyVecEnv
from stable_baselines3.common.monitor import Monitor

# ------------------------ Pygame ì´ˆê¸°í™” ------------------------
pygame.init()
screen = pygame.display.set_mode((400, 300))
pygame.display.set_caption("HIL Control Window")

# ------------------------ ë””ë ‰í† ë¦¬ ì„¤ì • ------------------------
MODEL_DIR = "sac_hil_model_v0_2"
LOG_DIR = "tensorboard_logs"
MODEL_PATH = os.path.join(MODEL_DIR, "sac_car_racing_best")
os.makedirs(MODEL_DIR, exist_ok=True)
os.makedirs(LOG_DIR, exist_ok=True)

# ------------------------ Seed ì„¤ì • ------------------------
SEED = 1

# ------------------------ í™˜ê²½ ìƒì„± í•¨ìˆ˜ ------------------------
def make_env():
    def _init():
        env = gym.make("CarRacing-v3", domain_randomize=False, render_mode="human")
        env = Monitor(env, filename=os.path.join(LOG_DIR, "SAC_HIL_seed1_v0_2.csv"))
        env.reset(seed=SEED)
        return env
    return _init

# ------------------------ í™˜ê²½ ìƒì„± ------------------------
env = DummyVecEnv([make_env()])
env.seed(SEED)

# ------------------------ ëª¨ë¸ ë¡œë“œ ë˜ëŠ” ìƒˆë¡œ ìƒì„± ------------------------
try:
    model = SAC.load(MODEL_PATH, env=env, tensorboard_log=LOG_DIR)
    print(f"âœ… ê¸°ì¡´ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {MODEL_PATH}")
except:
    print("ðŸš€ ìƒˆ ëª¨ë¸ ìƒì„± ì‹œìž‘")
    model = SAC(
        "CnnPolicy",
        env,
        learning_rate=3e-4,
        buffer_size=1_000_000,
        batch_size=256,
        tau=0.005,
        gamma=0.99,
        train_freq=1,
        gradient_steps=1,
        verbose=1,
        tensorboard_log=LOG_DIR
    )

# ------------------------ ì œì–´ ë³€ìˆ˜ ì´ˆê¸°í™” ------------------------
current_steering = 0.0
current_speed = 0.5

# ------------------------ HIL í•˜ì´í¼íŒŒë¼ë¯¸í„° ------------------------
initial_alpha = 0.9        # ì‚¬ëžŒ ê°œìž… ë¹„ìœ¨ ì´ˆê¸°ê°’
min_alpha = 0.0            # ìµœì†Œ ê°œìž… ë¹„ìœ¨
decay_rate = 0.1           # ê°œìž… ë¹„ìœ¨ ê°ì†Œ ì†ë„
max_human_steps = 50000    # ì‚¬ëžŒ ê°œìž… í—ˆìš© ìµœëŒ€ ìŠ¤í… ìˆ˜

# ------------------------ ì¸ê°„ ê°œìž… í•¨ìˆ˜ ------------------------
def get_human_action(original_action, step):
    global current_steering, current_speed
    keys = pygame.key.get_pressed()
    action = np.array(original_action, dtype=np.float32).reshape(-1)

    steer_step = 0.1         # ì¢Œìš° ì¡°í–¥ ë³€í™”ëŸ‰
    speed_step = 0.05        # ì „ì§„ ê°€ì† ë³€í™”ëŸ‰
    brake_step = 0.1         # ë¸Œë ˆì´í¬ ê°•ë„
    steering_recovery = 0.05 # ì¡°í–¥ ë³µì› ì†ë„

    if keys[pygame.K_LEFT]:
        current_steering -= steer_step
        action[2] = min(0.3, action[2] + brake_step)
    if keys[pygame.K_RIGHT]:
        current_steering += steer_step
        action[2] = min(0.3, action[2] + brake_step)
    if keys[pygame.K_UP]:
        current_speed += speed_step
        action[2] = 0.0
        if current_steering > 0:
            current_steering = max(0, current_steering - steering_recovery)
        elif current_steering < 0:
            current_steering = min(0, current_steering + steering_recovery)
    if keys[pygame.K_DOWN]:
        action[2] = 1.0
        current_speed *= 0.8
    if not any([keys[pygame.K_DOWN], keys[pygame.K_LEFT], keys[pygame.K_RIGHT]]):
        action[2] = max(0.0, action[2] - 0.05)

    current_steering = np.clip(current_steering, -1.0, 1.0)
    current_speed = np.clip(current_speed, 0.0, 1.0)
    action[2] = np.clip(action[2], 0.0, 1.0)

    alpha = max(min_alpha, initial_alpha - decay_rate * (step / max_human_steps)) if step < max_human_steps else 0.0
    action[0] = alpha * current_steering + (1 - alpha) * action[0]  # ì¡°í–¥ í˜¼í•©
    action[1] = alpha * current_speed + (1 - alpha) * action[1]     # ì†ë„ í˜¼í•©
    action[1] = np.clip(action[1], 0.0, 1.0)

    return action

# ------------------------ ê°œìž… ì—¬ë¶€ì— ë”°ë¼ í•™ìŠµ ìˆ˜í–‰ ------------------------
def train_if_human_intervened(step):
    global human_intervened
    if step < max_human_steps and step % 1000 == 0 and human_intervened:
        print(f"ðŸ“¢ Step {step}: ì‚¬ëžŒ ê°œìž… â†’ 1000 ìŠ¤í… í•™ìŠµ")
        model.learn(total_timesteps=1000, reset_num_timesteps=False)
        human_intervened = False

# ------------------------ ë©”ì¸ ë£¨í”„ ------------------------
obs = env.reset()
obs = obs.transpose(0, 3, 1, 2)
step = 0
total_timesteps = 1_000_000
human_intervened = False
# ë¸Œë ˆì´í¬ ê°ì§€ ë³€ìˆ˜
brake_duration = 0  

while step <= max_human_steps:
    pygame.event.pump()
    action = model.predict(obs, deterministic=True)[0]

    if step < max_human_steps and any(pygame.key.get_pressed()):
        action = get_human_action(action, step)
        human_intervened = True

    action = np.array(action).reshape(1, -1)
    result = env.step(action)

    if len(result) == 4:
        next_obs, reward, done, info = result
        terminated, truncated = done, False
    else:
        next_obs, reward, terminated, truncated, info = result


    done = terminated or truncated
    next_obs = next_obs.transpose(0, 3, 1, 2)

    # ë¦¬í”Œë ˆì´ ë²„í¼ì— transition ì¶”ê°€
    model.replay_buffer.add(obs, next_obs, action, [reward], [terminated], [{}])
    # if human_intervened:
    #     print(f"ë¦¬í”Œë ˆì´ ë²„í¼ ì¶”ê°€ë¨ | Step {step} | Action: {action} | Speed: {current_speed:.3f} | Brake: {action[0][2]:.3f}")
    #     print(f"í˜„ìž¬ ë²„í¼ í¬ê¸°: {model.replay_buffer.size()}")


    train_if_human_intervened(step)

    obs = next_obs
    step += 1
    env.render()

    print(f"Step {step} | Human: {human_intervened} | Action: {action}")

    if step == max_human_steps:
        print("ðŸ’¾ ëª¨ë¸ ì €ìž¥ (ì‚¬ëžŒ ê°œìž… ì¢…ë£Œ ì‹œì )")
        model.save(os.path.join(MODEL_DIR, "after_human_model.zip"))

        print("ðŸŽ¯ ì‚¬ëžŒ ê°œìž… ì§í›„, ì§‘ì¤‘ í•™ìŠµ ì‹œìž‘ (5ë§Œ ìŠ¤í…)")
        model.learn(total_timesteps=50000, reset_num_timesteps=False)
        model.save(os.path.join(MODEL_DIR, "after_human_learned_model.zip"))


    if done:
        current_steering, current_speed = 0.0, 0.0
        obs = env.reset()
        obs = obs.transpose(0, 3, 1, 2)

# ------------------------ ì‚¬ëžŒ ê°œìž… ì´í›„ ë°˜ë³µ í•™ìŠµ ------------------------
print("ðŸš€ ì‚¬ëžŒ ê°œìž… ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë°˜ë³µ í•™ìŠµ ì‹œìž‘")

model = SAC.load(os.path.join(MODEL_DIR, "after_human_learned_model.zip"), env=env, tensorboard_log=LOG_DIR)

print("ðŸ” ì‚¬ëžŒ ê°œìž… ë°ì´í„° ìž¬í•™ìŠµ (pre-train 5ë§Œ ìŠ¤í…)")
model.learn(total_timesteps=50000, reset_num_timesteps=False)

print("ðŸš€ ë³¸ í•™ìŠµ ì‹œìž‘ (900,000 ìŠ¤í…)")
model.learn(total_timesteps=900000, reset_num_timesteps=False)

# ------------------------ ìµœì¢… ëª¨ë¸ ì €ìž¥ ------------------------
model.save(MODEL_PATH)
print(f"âœ… í•™ìŠµ ì™„ë£Œ! ìµœì¢… ëª¨ë¸ ì €ìž¥ë¨ â†’ {MODEL_PATH}")

pygame.quit()