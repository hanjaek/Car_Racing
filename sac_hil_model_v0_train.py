import os
import gymnasium as gym
import pygame
import numpy as np
from stable_baselines3 import SAC
from stable_baselines3.common.vec_env import DummyVecEnv
from stable_baselines3.common.monitor import Monitor

# âœ… ëª¨ë¸ ë° ë¡œê·¸ ì €ì¥í•  í´ë” ì„¤ì •
MODEL_DIR = "basic_model_v0"
LOG_DIR = "tensorboard_logs"  
MODEL_PATH = os.path.join(MODEL_DIR, "sac_car_racing_best")

# í´ë”ê°€ ì—†ìœ¼ë©´ ìƒì„±
os.makedirs(MODEL_DIR, exist_ok=True)
os.makedirs(LOG_DIR, exist_ok=True)

# âœ… CarRacing í™˜ê²½ ìƒì„±
env = gym.make("CarRacing-v3", domain_randomize=False, render_mode="human")
env = Monitor(env)
env = DummyVecEnv([lambda: env])

# âœ… í‚¤ë³´ë“œ ì…ë ¥ ì„¤ì •
pygame.init()
screen = pygame.display.set_mode((400, 300))  # PyGame ì°½ (ë Œë”ë§ ì—†ì´ ì…ë ¥ë§Œ ë°›ìŒ)
pygame.display.set_caption("Human-in-the-Loop Controller")

# í‚¤ë³´ë“œ ì…ë ¥ì„ í–‰ë™ ê°’ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜
def get_human_action():
    keys = pygame.key.get_pressed()
    
    action = np.array([0.0, 0.0, 0.0])  # [steering, acceleration, brake]

    if keys[pygame.K_LEFT]:   # ì¢ŒíšŒì „
        action[0] = -1.0  
    if keys[pygame.K_RIGHT]:  # ìš°íšŒì „
        action[0] = 1.0  
    if keys[pygame.K_UP]:     # ê°€ì†
        action[1] = 1.0  
    if keys[pygame.K_DOWN]:   # ë¸Œë ˆì´í¬
        action[2] = 1.0  

    return action

# âœ… ê¸°ì¡´ ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸° or ìƒˆë¡œìš´ ëª¨ë¸ ìƒì„±
try:
    model = SAC.load(MODEL_PATH, env=env, tensorboard_log=LOG_DIR)
    print(f"âœ… ê¸°ì¡´ ëª¨ë¸ì„ ë¶ˆëŸ¬ì™€ì„œ ì¶”ê°€ í•™ìŠµí•©ë‹ˆë‹¤. ({MODEL_PATH})")
except:
    print("ğŸš€ ê¸°ì¡´ ëª¨ë¸ì´ ì—†ì–´ì„œ ìƒˆë¡œ í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤.")
    model = SAC(
        "CnnPolicy",
        env,
        learning_rate=3e-4,
        buffer_size=1000000,
        batch_size=256,
        tau=0.005,
        gamma=0.99,
        train_freq=1,
        gradient_steps=1,
        verbose=1,
        tensorboard_log=LOG_DIR
    )

# âœ… Human-in-the-Loop í•™ìŠµ ë£¨í”„
TIMESTEPS = 3000000
obs = env.reset()
human_override = False

for step in range(TIMESTEPS):
    pygame.event.pump()  # ì´ë²¤íŠ¸ ì²˜ë¦¬
    human_action = get_human_action()

    # ì‚¬ëŒì´ ì…ë ¥í–ˆëŠ”ì§€ ì²´í¬
    if np.any(human_action != 0.0):
        action = human_action  # ì‚¬ëŒì´ ì¡°ì‘í•œ í–‰ë™ì„ ì‚¬ìš©
        human_override = True
    else:
        action, _states = model.predict(obs, deterministic=True)  # AIê°€ í–‰ë™ ê²°ì •
        human_override = False

    obs, reward, done, info = env.step(action)

    # ì‚¬ëŒì´ ì¡°ì‘í•œ ë°ì´í„°ëŠ” ë³„ë„ ë²„í¼ì— ì €ì¥í•˜ì—¬ í•™ìŠµ ë°ì´í„°ë¡œ í™œìš© ê°€ëŠ¥
    if human_override:
        model.replay_buffer.add(obs, action, reward, done, obs)

    if done:
        obs = env.reset()

# âœ… ëª¨ë¸ ì €ì¥
model.save(MODEL_PATH)
print(f"ğŸ’¾ í•™ìŠµì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ëª¨ë¸ì´ '{MODEL_PATH}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
pygame.quit()
