import os
import gymnasium as gym
import numpy as np
import pygame  
from stable_baselines3 import SAC
from stable_baselines3.common.vec_env import DummyVecEnv
from stable_baselines3.common.monitor import Monitor

# Pygame ì´ˆê¸°í™”
pygame.init()
screen = pygame.display.set_mode((400, 300))  
pygame.display.set_caption("HIL Control Window")

# ëª¨ë¸ ë° ë¡œê·¸ ì €ì¥ í´ë” ì„¤ì •
MODEL_DIR = "sac_hil_model_v0"
LOG_DIR = "tensorboard_logs"
MODEL_PATH = os.path.join(MODEL_DIR, "sac_car_racing_best")

os.makedirs(MODEL_DIR, exist_ok=True)
os.makedirs(LOG_DIR, exist_ok=True)

# CarRacing í™˜ê²½ ìƒì„±
env = gym.make("CarRacing-v3", domain_randomize=False, render_mode="human")
env = Monitor(env)
env = DummyVecEnv([lambda: env])

# ê¸°ì¡´ ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸° or ìƒˆë¡œìš´ ëª¨ë¸ ìƒì„±
try:
    model = SAC.load(MODEL_PATH, env=env, tensorboard_log=LOG_DIR)
    print(f"ê¸°ì¡´ ëª¨ë¸ì„ ë¶ˆëŸ¬ì™€ì„œ ì¶”ê°€ í•™ìŠµí•©ë‹ˆë‹¤. ({MODEL_PATH})")
except:
    print("ğŸš€ ê¸°ì¡´ ëª¨ë¸ì´ ì—†ì–´ì„œ ìƒˆë¡œ í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤.")
    model = SAC(
        "CnnPolicy",
        env,
        learning_rate=3e-4,
        buffer_size=1000000,
        batch_size=256,
        tau=0.005,
        gamma=0.99,
        train_freq=1,
        gradient_steps=1,
        verbose=1,
        tensorboard_log=LOG_DIR
    )

# ì´ˆê¸° ì†ë„ ë° ë°©í–¥ ë³€ìˆ˜
current_steering = 0.0  # í˜„ì¬ ì¡°í–¥ ê°’ (-1.0 ~ 1.0)
current_speed = 0.0     # í˜„ì¬ ì†ë„ (0.0 ~ 1.0)

# í‚¤ì…ë ¥ì„ í†µí•œ ì¸ê°„ ê°œì…
def get_human_action(original_action):
    global current_steering, current_speed
    
    keys = pygame.key.get_pressed()
    action = np.array(original_action, dtype=np.float32).reshape(-1)  

    steer_step = 0.1  # ì¡°í–¥ ì¡°ì ˆ ê°•ë„
    speed_step = 0.05  # ì†ë„ ì¡°ì ˆ ê°•ë„
    steering_recovery = 0.05  # ê°€ì† ì‹œ ì§ì§„ íšŒë³µ ê°•ë„

    # ì™¼ìª½/ì˜¤ë¥¸ìª½ ë°©í–¥í‚¤ ëˆ„ë¥¼ ë•Œë§ˆë‹¤ ì ì§„ì  ì¡°ì •
    if keys[pygame.K_LEFT]:  
        current_steering -= steer_step  # ì¢ŒíšŒì „
    if keys[pygame.K_RIGHT]:  
        current_steering += steer_step  # ìš°íšŒì „

    # ìœ„ ë°©í–¥í‚¤(ê°€ì†) ëˆ„ë¥´ë©´ ì ì§„ì ìœ¼ë¡œ ì†ë„ ì¦ê°€ (ë²„íŠ¼ ë–¼ë„ ìœ ì§€)
    if keys[pygame.K_UP]:  
        current_speed += speed_step
        # ê°€ì† ì¤‘ì—ëŠ” ì¡°í–¥ì„ ì ì§„ì ìœ¼ë¡œ 0ìœ¼ë¡œ ë³µê·€ (ì§ì§„)
        if current_steering > 0:
            current_steering = max(0, current_steering - steering_recovery)
        elif current_steering < 0:
            current_steering = min(0, current_steering + steering_recovery)

    # ì•„ë˜ ë°©í–¥í‚¤(ê°ì†) ëˆ„ë¥´ë©´ ì ì§„ì ìœ¼ë¡œ ì†ë„ ê°ì†Œ (ë²„íŠ¼ ë–¼ë„ ìœ ì§€)
    if keys[pygame.K_DOWN]:  
        current_speed -= speed_step  

    # ê°’ ë²”ìœ„ ì œí•œ
    current_steering = np.clip(current_steering, -1.0, 1.0)
    current_speed = np.clip(current_speed, 0.0, 1.0)  

    # ì¡°ì‘ëœ ì•¡ì…˜ ì ìš©
    action[0] = current_steering  # ì¡°í–¥ ìœ ì§€
    action[1] = current_speed  # ì†ë„ ìœ ì§€
    action[2] = 0.0  # ë¸Œë ˆì´í¬ í•´ì œ

    return action


# HIL í•™ìŠµ ë£¨í”„ (300ë§Œ ìŠ¤í…)
obs = env.reset()
obs = obs.transpose(0, 3, 1, 2)  # ğŸš€ (1, 96, 96, 3) -> (1, 3, 96, 96) ë³€í™˜
done = False
total_timesteps = 3000000
step = 0

update_on = False  # ğŸš€ ì‚¬ëŒì´ ê°œì…í–ˆëŠ”ì§€ ì—¬ë¶€ë¥¼ ì¶”ì í•˜ëŠ” ë³€ìˆ˜

while step < total_timesteps:
    pygame.event.pump()  

    human_override = False  
    action = model.predict(obs, deterministic=True)[0]  

    if any(pygame.key.get_pressed()):  
        action = get_human_action(action)
        human_override = True  
        update_on = True  # ğŸš€ ì‚¬ëŒì´ ê°œì…í–ˆìœ¼ë¯€ë¡œ í•™ìŠµì„ ì˜ˆì•½

    action = np.array(action).reshape(1, -1)  

    # í™˜ê²½ ì—…ë°ì´íŠ¸ (Gymnasium step() ë°˜í™˜ê°’ ì²˜ë¦¬)
    step_result = env.step(action)

    if len(step_result) == 4:  
        next_obs, reward, done, info = step_result
        terminated, truncated = done, False  
    elif len(step_result) == 5:  
        next_obs, reward, terminated, truncated, info = step_result
    else:
        raise ValueError(f"Unexpected number of return values from env.step(action): {len(step_result)}")

    done = terminated or truncated

    # **obsì™€ next_obsë¥¼ (1, 3, 96, 96)ë¡œ ë³€í™˜**
    next_obs = next_obs.transpose(0, 3, 1, 2)  

    # âœ… SAC ëª¨ë¸ì˜ ì£¼í–‰ ë°ì´í„°ë„ í•™ìŠµ ë°ì´í„°ë¡œ ì¶”ê°€
    model.replay_buffer.add(
        np.array(obs),  
        np.array(next_obs),  
        np.array(action),  
        np.array([reward]),  
        np.array([terminated]),  
        [{}]  
    )

    # âœ… 1000 ìŠ¤í…ë§ˆë‹¤ í•™ìŠµ ì‹¤í–‰ (ì‚¬ëŒì´ í•œ ë²ˆì´ë¼ë„ ê°œì…í–ˆìœ¼ë©´ ì‹¤í–‰)
    if update_on and step % 1000 == 0:
        print(f"ğŸ“¢ Step {step}: Human Override detected earlier, training for 1000 steps...")
        model.learn(total_timesteps=1000)
        update_on = False  # ğŸš€ í•™ìŠµ ì‹¤í–‰ í›„ ë¦¬ì…‹

    obs = next_obs  
    step += 1
    env.render()

    print(f"Step: {step}, Human Override: {human_override}, Action: {action}")



# ëª¨ë¸ ì €ì¥
model.save(MODEL_PATH)
print(f"ğŸ’¾ í•™ìŠµì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ëª¨ë¸ì´ '{MODEL_PATH}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

pygame.quit()