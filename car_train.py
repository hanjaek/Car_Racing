import os
import gymnasium as gym
import numpy as np
import pandas as pd
from stable_baselines3 import SAC
from stable_baselines3.common.vec_env import DummyVecEnv
from stable_baselines3.common.monitor import Monitor

# ëª¨ë¸ ë° ë¡œê·¸ ì €ì¥í•  í´ë” ê²½ë¡œ ì„¤ì •
MODEL_DIR = "model"
LOG_FILE = os.path.join(MODEL_DIR, "training_log.csv")
MODEL_PATH = os.path.join(MODEL_DIR, "sac_car_racing_best")

# ëª¨ë¸ í´ë”ê°€ ì—†ìœ¼ë©´ ìƒì„±
os.makedirs(MODEL_DIR, exist_ok=True)

# CarRacing í™˜ê²½ ìƒì„±
env = gym.make("CarRacing-v3", domain_randomize=False, render_mode="rgb_array")

# Monitorë¡œ í™˜ê²½ ê°ì‹œ (ë¡œê·¸ ì €ì¥)
env = Monitor(env)

# ë²¡í„° í™˜ê²½ìœ¼ë¡œ ë˜í•‘ (SAC í•™ìŠµ ì•ˆì •ì„±ì„ ìœ„í•´ í•„ìš”)
env = DummyVecEnv([lambda: env])

# í•™ìŠµëœ ëª¨ë¸ì´ ìˆìœ¼ë©´ ë¶ˆëŸ¬ì™€ì„œ ì¶”ê°€ í•™ìŠµ, ì—†ìœ¼ë©´ ìƒˆ ëª¨ë¸ í•™ìŠµ
try:
    model = SAC.load(MODEL_PATH, env=env)
    print(f"âœ… ê¸°ì¡´ ëª¨ë¸ì„ ë¶ˆëŸ¬ì™€ì„œ ì¶”ê°€ í•™ìŠµí•©ë‹ˆë‹¤. ({MODEL_PATH})")
except:
    print("ğŸš€ ê¸°ì¡´ ëª¨ë¸ì´ ì—†ì–´ì„œ ìƒˆë¡œ í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤.")
    model = SAC(
        "CnnPolicy",
        env,
        learning_rate=3e-4,
        buffer_size=100000,
        batch_size=64,
        tau=0.005,
        gamma=0.99,
        train_freq=4,
        gradient_steps=2,
        verbose=1
    )

# í•™ìŠµ ë¡œê·¸ ì €ì¥ì„ ìœ„í•œ ë°ì´í„° ë¦¬ìŠ¤íŠ¸
log_data = []

# í•™ìŠµ ìˆ˜í–‰ (ìµœì†Œ 1000ë§Œ ìŠ¤í…)
for step in range(0, 10000000, 10000):  # 10,000 ìŠ¤í…ë§ˆë‹¤ ì €ì¥
    model.learn(total_timesteps=10000, reset_num_timesteps=False)
    
    # Gym Monitorê°€ ë¡œê·¸ íŒŒì¼ì— ë³´ìƒì„ ê¸°ë¡í•˜ë¯€ë¡œ, info ë”•ì…”ë„ˆë¦¬ì—ì„œ ë³´ìƒ ê°€ì ¸ì˜¤ê¸°
    episode_rewards = []
    for _ in range(10):  # ìµœê·¼ 10 ì—í”¼ì†Œë“œë§Œ ì €ì¥
        obs, _ = env.reset()
        done = False
        total_reward = 0
        while not done:
            action, _ = model.predict(obs, deterministic=True)
            obs, reward, terminated, truncated, info = env.step(action)
            total_reward += reward
            done = terminated or truncated
        episode_rewards.append(total_reward)

    # í‰ê·  ë³´ìƒ ê³„ì‚°
    ep_rew_mean = np.mean(episode_rewards) if episode_rewards else None

    # í•™ìŠµ ìƒíƒœ ì €ì¥
    training_info = {
        "total_timesteps": step + 10000,
        "ep_rew_mean": ep_rew_mean,
        "actor_loss": model.actor.optimizer.param_groups[0]['lr'],
        "critic_loss": model.critic.optimizer.param_groups[0]['lr'],
        "ent_coef": model.ent_coef_optimizer.param_groups[0]['lr']
    }
    
    log_data.append(training_info)

    # ë¡œê·¸ë¥¼ CSV íŒŒì¼ë¡œ ì €ì¥
    df = pd.DataFrame(log_data)
    df.to_csv(LOG_FILE, index=False)
    print(f"ğŸ“Š ë¡œê·¸ ì €ì¥ ì™„ë£Œ: {LOG_FILE}")

# ëª¨ë¸ ì €ì¥ (model í´ë” ì•ˆì— ì €ì¥ë¨)
model.save(MODEL_PATH)
print(f"ğŸ’¾ í•™ìŠµì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ëª¨ë¸ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤. ({MODEL_PATH})")
