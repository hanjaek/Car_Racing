import os
import time
import gymnasium as gym
import numpy as np
from stable_baselines3 import SAC
from stable_baselines3.common.vec_env import DummyVecEnv
from stable_baselines3.common.monitor import Monitor
from torch.utils.tensorboard import SummaryWriter

# ğŸ“Œ ëª¨ë¸ & ë¡œê·¸ ì €ì¥ ê²½ë¡œ
MODEL_DIR = "basic_model_v0"
MODEL_PATH = os.path.join(MODEL_DIR, "sac_car_racing_best")
LOG_DIR = "tensorboard_logs/basic_model_v0_logs"

# í´ë” ìƒì„±
os.makedirs(MODEL_DIR, exist_ok=True)
os.makedirs(LOG_DIR, exist_ok=True)

# TensorBoard ê¸°ë¡ê¸° ìƒì„±
writer = SummaryWriter(LOG_DIR)

# CarRacing í™˜ê²½ ìƒì„±
env = gym.make("CarRacing-v3", domain_randomize=False, render_mode="rgb_array")
env = Monitor(env)
env = DummyVecEnv([lambda: env])

# ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°
try:
    model = SAC.load(MODEL_PATH, env=env)
    print(f"âœ… ê¸°ì¡´ ëª¨ë¸ì„ ë¶ˆëŸ¬ì™€ì„œ ì¶”ê°€ í•™ìŠµí•©ë‹ˆë‹¤. ({MODEL_PATH})")
except:
    print("ğŸš€ ê¸°ì¡´ ëª¨ë¸ì´ ì—†ì–´ì„œ ìƒˆë¡œ í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤.")
    model = SAC(
        "CnnPolicy",
        env,
        learning_rate=3e-4,
        buffer_size=1000000,
        batch_size=256,
        tau=0.005,
        gamma=0.99,
        train_freq=1,
        gradient_steps=1,
        verbose=1
    )

# ğŸ† í•™ìŠµ ëª¨ë‹ˆí„°ë§ì„ ìœ„í•œ ì§€í‘œ ê¸°ë¡ í•¨ìˆ˜
def log_training_metrics(model, writer, num_episodes=10):
    episode_rewards = []
    episode_lengths = []
    
    for ep in range(num_episodes):
        obs, _ = env.reset()
        done = [False]
        total_reward = 0
        step = 0

        while not done[0]:  
            action, _ = model.predict(obs, deterministic=True)
            obs, reward, done, info = env.step(action)
            total_reward += reward[0]
            step += 1

        episode_rewards.append(total_reward)
        episode_lengths.append(step)

        # ğŸŸ¢ TensorBoardì— ë‹¤ì–‘í•œ ì§€í‘œ ê¸°ë¡
        writer.add_scalar("Rewards/Total Reward per Episode", total_reward, ep)
        writer.add_scalar("Episode_Length/Steps per Episode", step, ep)

        if ep % 10 == 0:
            writer.flush()

        print(f"Episode {ep+1}: Total Reward = {total_reward}, Steps = {step}")

    # ì¶”ê°€ë¡œ í‰ê·  ë³´ìƒë„ ê¸°ë¡
    mean_reward = np.mean(episode_rewards)
    writer.add_scalar("Rewards/Mean Reward", mean_reward)

    return episode_rewards

# ğŸ”¥ í•™ìŠµ ì§„í–‰ (í•™ìŠµ ì¤‘ì—ë„ ë³´ìƒ ì €ì¥)
for i in range(10):  
    model.learn(total_timesteps=100000, reset_num_timesteps=False)
    model.save(MODEL_PATH)
    print(f"ğŸ’¾ {((i+1)*100000)} ìŠ¤í… ì§„í–‰ë¨. ëª¨ë¸ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤. ({MODEL_PATH})")

    # í•™ìŠµ ì¤‘ì—ë„ ì§€í‘œ ê¸°ë¡
    log_training_metrics(model, writer, num_episodes=10)

# TensorBoard ê¸°ë¡ ì¢…ë£Œ
writer.close()
print(f"ğŸ“Š TensorBoard ë¡œê·¸ê°€ '{LOG_DIR}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤. ì‹¤í–‰í•˜ë ¤ë©´:")
print(f"tensorboard --logdir={LOG_DIR}")
