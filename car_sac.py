import gymnasium as gym
from stable_baselines3 import SAC
from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize
import os
import matplotlib

# âœ… Matplotlib ë°±ì—”ë“œ ë¹„í™œì„±í™” (ê²½ê³  ë©”ì‹œì§€ ë°©ì§€)
matplotlib.use('Agg')

class CustomCarRacingEnv(gym.Wrapper):
    def __init__(self, env):
        super(CustomCarRacingEnv, self).__init__(env)

    def step(self, action):
        obs, reward, terminated, truncated, info = self.env.step(action)

        # ğŸ¯ ì‚¬ìš©ì ì •ì˜ ë³´ìƒ ì ìš©
        reward = self.custom_reward(obs, action)

        return obs, reward, terminated, truncated, info

    def custom_reward(self, obs, action):
        """
        ë„ë¡œ ì¤‘ì‹¬ì„ ìœ ì§€í•˜ê³  ì •ìƒì ì¸ ì£¼í–‰ì„ ìœ ë„í•˜ëŠ” ë³´ìƒ í•¨ìˆ˜
        """
        speed_reward = obs[4]  # ì†ë„ ìœ ì§€ ë³´ìƒ
        track_position = obs[1]  # íŠ¸ë™ ì¤‘ì‹¬ì—ì„œì˜ ê±°ë¦¬
        angle = obs[3]  # ì°¨ëŸ‰ ë°©í–¥ê³¼ ë„ë¡œ ë°©í–¥ì˜ ì°¨ì´

        # ë„ë¡œ ì¤‘ì‹¬ ìœ ì§€ ë³´ìƒ
        track_reward = 1.0 - abs(track_position)

        # ë„ë¡œ ì´íƒˆ íŒ¨ë„í‹°
        off_road_penalty = -10 if abs(track_position) > 0.9 else 0

        # ì°¨ëŸ‰ ë°©í–¥ íŒ¨ë„í‹°
        angle_penalty = -abs(angle)

        # ì´ ë³´ìƒ ê³„ì‚°
        total_reward = speed_reward + track_reward + off_road_penalty + angle_penalty
        return total_reward

# ğŸ¯ í™˜ê²½ ê°ì‹¸ê¸° (VecEnv ì ìš©)
env = DummyVecEnv([lambda: CustomCarRacingEnv(gym.make("CarRacing-v3", render_mode="human"))])
env = VecNormalize(env, norm_obs=True, norm_reward=True, clip_obs=10.0)  # ê´€ì°°ê°’ê³¼ ë³´ìƒ ì •ê·œí™”

# âœ… ì €ì¥ëœ ëª¨ë¸ê³¼ í™˜ê²½ì´ ìˆëŠ”ì§€ í™•ì¸ í›„ ë¶ˆëŸ¬ì˜¤ê¸°
if os.path.exists("sac_CarRacing.zip") and os.path.exists("sac_CarRacing_env.pkl"):
    env = VecNormalize.load("sac_CarRacing_env.pkl", env)  # âœ… ì €ì¥ëœ í™˜ê²½ ë¶ˆëŸ¬ì˜¤ê¸°
    model = SAC.load("sac_CarRacing", env=env)  # âœ… ê¸°ì¡´ ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°
    print("âœ… ê¸°ì¡´ í•™ìŠµëœ ëª¨ë¸ê³¼ í™˜ê²½ ë¶ˆëŸ¬ì™€ì„œ ì¶”ê°€ í•™ìŠµ ì§„í–‰")
else:
    print("ğŸš€ ì €ì¥ëœ ëª¨ë¸ì´ ì—†ê±°ë‚˜ í™˜ê²½ì´ ì†ìƒë¨. ìƒˆë¡œ í•™ìŠµ ì‹œì‘")
    model = SAC("MlpPolicy", env, verbose=1)

# ì¶”ê°€ í•™ìŠµ ì§„í–‰ (ì´ì „ í•™ìŠµ ë‚´ìš©ì„ ìœ ì§€í•˜ë©° í•™ìŠµ)
model.learn(total_timesteps=50000, log_interval=4)

# âœ… ëª¨ë¸ê³¼ í™˜ê²½ ì €ì¥
model.save("sac_CarRacing")
env.save("sac_CarRacing_env.pkl")  # âœ… í™˜ê²½ ì •ë³´ë„ í•¨ê»˜ ì €ì¥

# âœ… í˜„ì¬ í™˜ê²½ì˜ ë Œë” ëª¨ë“œ í™•ì¸
print("âœ… í˜„ì¬ í™˜ê²½ ë Œë” ëª¨ë“œ:", env.get_attr('render_mode'))

# âœ… ì£¼í–‰ í™”ë©´ ë³´ê¸° (VecEnv ë‚´ë¶€ í™˜ê²½ ì§ì ‘ ë Œë”ë§)
obs, info = env.reset()
while True:
    env.envs[0].render()  # âœ… VecEnv ë‚´ë¶€ í™˜ê²½ì—ì„œ ì§ì ‘ render() í˜¸ì¶œ
    action, _states = model.predict(obs, deterministic=True)
    obs, reward, terminated, truncated, info = env.step(action)
    if terminated or truncated:
        obs, info = env.reset()
