import gymnasium as gym
from stable_baselines3 import SAC
from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize
import os

class CustomCarRacingEnv(gym.Wrapper):
    def __init__(self, env):
        super(CustomCarRacingEnv, self).__init__(env)

    def step(self, action):
        obs, reward, terminated, truncated, info = self.env.step(action)

        # ğŸ¯ ì‚¬ìš©ì ì •ì˜ ë³´ìƒ ì ìš©
        reward = self.custom_reward(obs, action)

        return obs, reward, terminated, truncated, info

    def custom_reward(self, obs, action):
        """
        ë„ë¡œ ì¤‘ì‹¬ì„ ìœ ì§€í•˜ê³  ì •ìƒì ì¸ ì£¼í–‰ì„ ìœ ë„í•˜ëŠ” ë³´ìƒ í•¨ìˆ˜
        """
        speed_reward = obs[4]  # ì†ë„ ìœ ì§€ ë³´ìƒ
        track_position = obs[1]  # íŠ¸ë™ ì¤‘ì‹¬ì—ì„œì˜ ê±°ë¦¬
        angle = obs[3]  # ì°¨ëŸ‰ ë°©í–¥ê³¼ ë„ë¡œ ë°©í–¥ì˜ ì°¨ì´

        # ë„ë¡œ ì¤‘ì‹¬ ìœ ì§€ ë³´ìƒ
        track_reward = 1.0 - abs(track_position)

        # ë„ë¡œ ì´íƒˆ íŒ¨ë„í‹°
        off_road_penalty = -10 if abs(track_position) > 0.9 else 0

        # ì°¨ëŸ‰ ë°©í–¥ íŒ¨ë„í‹°
        angle_penalty = -abs(angle)

        # ì´ ë³´ìƒ ê³„ì‚°
        total_reward = speed_reward + track_reward + off_road_penalty + angle_penalty
        return total_reward

# ğŸ¯ í™˜ê²½ ê°ì‹¸ê¸° (VecEnv ì ìš©)
env = DummyVecEnv([lambda: CustomCarRacingEnv(gym.make("CarRacing-v3", render_mode="human"))])
env = VecNormalize(env, norm_obs=True, norm_reward=True, clip_obs=10.0)  # ê´€ì°°ê°’ê³¼ ë³´ìƒ ì •ê·œí™”

# ì €ì¥ëœ ëª¨ë¸ì´ ìˆëŠ”ì§€ í™•ì¸ í›„ ë¶ˆëŸ¬ì˜¤ê¸°
if os.path.exists("sac_CarRacing.zip"):
    model = SAC.load("sac_CarRacing", env=env)  # ê¸°ì¡´ ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°
    print("âœ… ê¸°ì¡´ í•™ìŠµëœ ëª¨ë¸ ë¶ˆëŸ¬ì™€ì„œ ì¶”ê°€ í•™ìŠµ ì§„í–‰")
else:
    model = SAC("MlpPolicy", env, verbose=1)  # ìƒˆë¡œìš´ ëª¨ë¸ ìƒì„±
    print("ğŸš€ ì €ì¥ëœ ëª¨ë¸ì´ ì—†ì–´ ìƒˆë¡œìš´ ëª¨ë¸ì„ í•™ìŠµ ì‹œì‘")

# ì¶”ê°€ í•™ìŠµ ì§„í–‰ (ì´ì „ í•™ìŠµ ë‚´ìš©ì„ ìœ ì§€í•˜ë©° í•™ìŠµ)
model.learn(total_timesteps=50000, log_interval=4)

# ëª¨ë¸ ì €ì¥ (í•™ìŠµëœ ë‚´ìš© ìœ ì§€)
model.save("sac_CarRacing")

# í…ŒìŠ¤íŠ¸ ì‹¤í–‰
obs = env.reset()
while True:
    action, _states = model.predict(obs, deterministic=True)
    obs, reward, terminated, truncated, info = env.step(action)
    if terminated or truncated:
        obs = env.reset()
