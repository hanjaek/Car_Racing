import os
import gymnasium as gym
import numpy as np
import pygame  # âœ… í‚¤ë³´ë“œ ì…ë ¥ì„ ì²˜ë¦¬í•˜ê¸° ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬
from stable_baselines3 import SAC
from stable_baselines3.common.vec_env import DummyVecEnv
from stable_baselines3.common.monitor import Monitor

# âœ… Pygame ì´ˆê¸°í™” (HILì„ ìœ„í•œ í‚¤ ì…ë ¥ ì²˜ë¦¬)
pygame.init()
screen = pygame.display.set_mode((400, 300))  # Pygame ì°½ (ì‹¤ì œ ê²Œì„ í™”ë©´ê³¼ëŠ” ë¬´ê´€)
pygame.display.set_caption("HIL Control Window")

# âœ… ëª¨ë¸ ë° ë¡œê·¸ ì €ì¥í•  í´ë” ì„¤ì •
MODEL_DIR = "sac_hil_model_v0"
LOG_DIR = "tensorboard_logs"
MODEL_PATH = os.path.join(MODEL_DIR, "sac_car_racing_best")

# í´ë”ê°€ ì—†ìœ¼ë©´ ìƒì„±
os.makedirs(MODEL_DIR, exist_ok=True)
os.makedirs(LOG_DIR, exist_ok=True)

# âœ… CarRacing í™˜ê²½ ìƒì„±
env = gym.make("CarRacing-v3", domain_randomize=False, render_mode="human")  # ğŸš€ í™”ë©´ ì¶œë ¥ ê°€ëŠ¥í•˜ê²Œ ë³€ê²½
env = Monitor(env)
env = DummyVecEnv([lambda: env])

# âœ… ê¸°ì¡´ ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸° or ìƒˆë¡œìš´ ëª¨ë¸ ìƒì„±
try:
    model = SAC.load(MODEL_PATH, env=env, tensorboard_log=LOG_DIR)
    print(f"âœ… ê¸°ì¡´ ëª¨ë¸ì„ ë¶ˆëŸ¬ì™€ì„œ ì¶”ê°€ í•™ìŠµí•©ë‹ˆë‹¤. ({MODEL_PATH})")
except:
    print("ğŸš€ ê¸°ì¡´ ëª¨ë¸ì´ ì—†ì–´ì„œ ìƒˆë¡œ í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤.")
    model = SAC(
        "CnnPolicy",
        env,
        learning_rate=3e-4,
        buffer_size=1000000,
        batch_size=256,
        tau=0.005,
        gamma=0.99,
        train_freq=1,
        gradient_steps=1,
        verbose=1,
        tensorboard_log=LOG_DIR
    )

# âœ… í‚¤ ì…ë ¥ì„ ë°›ì•„ ì‚¬ëŒì´ ê°œì…í•  ìˆ˜ ìˆë„ë¡ í•˜ëŠ” í•¨ìˆ˜
def get_human_action(original_action):
    keys = pygame.key.get_pressed()
    
    # âœ… ëª¨ë¸ì´ ì˜ˆì¸¡í•œ ê¸°ì¡´ í–‰ë™ì„ ê¸°ë°˜ìœ¼ë¡œ ì¡°ì • (NumPy ë°°ì—´ë¡œ ë³€í™˜)
    action = np.array(original_action, dtype=np.float32)
    step = 0.1  # í‚¤ ì…ë ¥ì— ë”°ë¥¸ ì¡°ì ˆ ê°•ë„ (ë¶€ë“œëŸ¬ìš´ ì¡°ì‘)
    
    if keys[pygame.K_LEFT]:   # â† ì™¼ìª½ ë°©í–¥í‚¤
        action[0] -= step
    if keys[pygame.K_RIGHT]:  # â†’ ì˜¤ë¥¸ìª½ ë°©í–¥í‚¤
        action[0] += step
    if keys[pygame.K_UP]:     # â†‘ ê°€ì†
        action[1] += step
    if keys[pygame.K_DOWN]:   # â†“ ë¸Œë ˆì´í¬
        action[2] += step

    # âœ… NumPy ë°°ì—´ í˜•íƒœ ìœ ì§€í•˜ë©´ì„œ ê°’ ì œí•œ
    action = np.clip(action, [-1.0, 0.0, 0.0], [1.0, 1.0, 1.0])

    return action

# âœ… HIL í•™ìŠµ ë£¨í”„ (300ë§Œ ìŠ¤í…)
obs = env.reset()
done = False
total_timesteps = 3000000
step = 0

while step < total_timesteps:
    pygame.event.pump()  # í‚¤ë³´ë“œ ì…ë ¥ì„ ê°±ì‹ 

    human_override = False  # ì‚¬ëŒì´ ê°œì…í–ˆëŠ”ì§€ ì—¬ë¶€
    action = model.predict(obs, deterministic=True)[0]  # ê¸°ë³¸ì ìœ¼ë¡œ ëª¨ë¸ í–‰ë™ ì‚¬ìš©

    if any(pygame.key.get_pressed()):  # ì‚¬ëŒì´ í‚¤ë¥¼ ëˆ„ë¥´ë©´ HIL ê°œì…
        action = get_human_action(action)
        human_override = True  # ì‚¬ëŒì´ ê°œì…í–ˆìŒì„ í‘œì‹œ

    # âœ… í™˜ê²½ ì—…ë°ì´íŠ¸ (Gymnasium step() ë°˜í™˜ê°’ ì²˜ë¦¬)
    step_result = env.step(action)

    if len(step_result) == 4:  # (next_obs, reward, done, info) ë°˜í™˜í•˜ëŠ” ê²½ìš°
        next_obs, reward, done, info = step_result
        terminated, truncated = done, False  # `done`ì„ terminatedë¡œ ì‚¬ìš©í•˜ê³ , truncatedëŠ” Falseë¡œ ì„¤ì •
    elif len(step_result) == 5:  # (next_obs, reward, terminated, truncated, info) ë°˜í™˜í•˜ëŠ” ê²½ìš°
        next_obs, reward, terminated, truncated, info = step_result
    else:
        raise ValueError(f"Unexpected number of return values from env.step(action): {len(step_result)}")

    done = terminated or truncated

    # ì‚¬ëŒì´ ê°œì…í•œ ê²½ìš°ë§Œ ëª¨ë¸ í•™ìŠµ ë°ì´í„°ë¡œ ì¶”ê°€
    if human_override:
        model.replay_buffer.add(obs, next_obs, action, reward, terminated, [{}])

    obs = next_obs  # ë‹¤ìŒ ìƒíƒœë¡œ ì—…ë°ì´íŠ¸
    step += 1
    env.render()

# âœ… ëª¨ë¸ ì €ì¥
model.save(MODEL_PATH)
print(f"ğŸ’¾ í•™ìŠµì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ëª¨ë¸ì´ '{MODEL_PATH}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

pygame.quit()
