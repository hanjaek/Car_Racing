import os
import gymnasium as gym
from stable_baselines3 import SAC
from stable_baselines3.common.vec_env import DummyVecEnv
from stable_baselines3.common.monitor import Monitor
from stable_baselines3.common.callbacks import BaseCallback
from torch.utils.tensorboard import SummaryWriter

# âœ… TensorBoard ì½œë°± (ì—¬ëŸ¬ ê°€ì§€ í•™ìŠµ ì§€í‘œ ê¸°ë¡)
class TensorBoardCallback(BaseCallback):
    def __init__(self, log_dir):
        super().__init__()
        self.writer = SummaryWriter(log_dir)

    def _on_step(self) -> bool:
        # ë§¤ 1000 ìŠ¤í…ë§ˆë‹¤ ê¸°ë¡
        if self.n_calls % 1000 == 0:
            # SAC ëª¨ë¸ì—ì„œ ì†ì‹¤(loss) ê°’ ê°€ì ¸ì˜¤ê¸°
            actor_loss = self.model.actor.optimizer.param_groups[0]['lr']
            critic_loss = self.model.critic.optimizer.param_groups[0]['lr']
            entropy_loss = self.model.ent_coef_optimizer.param_groups[0]['lr']
            episode_reward = self.locals['rewards'][0]

            # TensorBoardì— ê¸°ë¡
            self.writer.add_scalar("Loss/Actor Loss", actor_loss, self.num_timesteps)
            self.writer.add_scalar("Loss/Critic Loss", critic_loss, self.num_timesteps)
            self.writer.add_scalar("Loss/Entropy Loss", entropy_loss, self.num_timesteps)
            self.writer.add_scalar("Rewards/Episode Reward", episode_reward, self.num_timesteps)
            
        return True

# âœ… ëª¨ë¸ ë° ë¡œê·¸ ì €ì¥í•  í´ë” ì„¤ì •
MODEL_DIR = "basic_model_v0"
LOG_DIR = "tensorboard_logs/basic_model_v0"
MODEL_PATH = os.path.join(MODEL_DIR, "sac_car_racing_best")

# í´ë”ê°€ ì—†ìœ¼ë©´ ìƒì„±
os.makedirs(MODEL_DIR, exist_ok=True)
os.makedirs(LOG_DIR, exist_ok=True)

# âœ… CarRacing í™˜ê²½ ìƒì„±
env = gym.make("CarRacing-v3", domain_randomize=False, render_mode="rgb_array")
env = Monitor(env)
env = DummyVecEnv([lambda: env])

# âœ… ê¸°ì¡´ ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸° or ìƒˆë¡œìš´ ëª¨ë¸ ìƒì„±
try:
    model = SAC.load(MODEL_PATH, env=env, tensorboard_log=LOG_DIR)
    print(f"âœ… ê¸°ì¡´ ëª¨ë¸ì„ ë¶ˆëŸ¬ì™€ì„œ ì¶”ê°€ í•™ìŠµí•©ë‹ˆë‹¤. ({MODEL_PATH})")
except:
    print("ğŸš€ ê¸°ì¡´ ëª¨ë¸ì´ ì—†ì–´ì„œ ìƒˆë¡œ í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤.")
    model = SAC(
        "CnnPolicy",
        env,
        learning_rate=3e-4,
        buffer_size=1000000,
        batch_size=256,
        tau=0.005,
        gamma=0.99,
        train_freq=1,
        gradient_steps=1,
        verbose=1,
        tensorboard_log=LOG_DIR
    )

# âœ… í•™ìŠµ ìˆ˜í–‰ (100ë§Œ ìŠ¤í…)
model.learn(total_timesteps=1000000, callback=TensorBoardCallback(LOG_DIR))

# âœ… ëª¨ë¸ ì €ì¥
model.save(MODEL_PATH)
print(f"ğŸ’¾ í•™ìŠµì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ëª¨ë¸ì´ '{MODEL_PATH}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
